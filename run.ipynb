{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "                             [--lr LR] [--conv_thresh CONV_THRESH]\n",
      "                             [--save_freq SAVE_FREQ] [--train] [--load]\n",
      "                             [--path PATH] [--note NOTE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/JMCooper/Library/Jupyter/runtime/kernel-a3e375bc-5889-4701-a6d7-3cf9442814da.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "from O365 import Account\n",
    "import argparse\n",
    "import time\n",
    "import atexit\n",
    "\n",
    "torch.set_printoptions(precision=4, linewidth=300)\n",
    "np.set_printoptions(precision=4, linewidth=300)\n",
    "\n",
    "################################################ SET UP SEED AND ARGS AND EXIT HANDLER\n",
    "\n",
    "seed = 42\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--lr', type=float, default=0.001)\n",
    "parser.add_argument('--conv_thresh', type=float, default=0.01)\n",
    "parser.add_argument('--save_freq', default=100, type=int)\n",
    "parser.add_argument('--train', default=False, action='store_true')\n",
    "parser.add_argument('--load', default=False, action='store_true')\n",
    "parser.add_argument('--path', default='./lc_net.pth')\n",
    "parser.add_argument('--note', default='')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "time_taken = 0\n",
    "epoch_loss = 0\n",
    "last_batch_loss = 0\n",
    "epoch = 0\n",
    "\n",
    "def save_results():\n",
    "    results = {'time': time_taken, 'last_batch_loss':last_batch_loss, 'epoch_loss': epoch_loss, 'epoch': epoch}\n",
    "    results = {**results, **vars(args)}\n",
    "    print(results)\n",
    "    results = pd.DataFrame(results, index=[0])\n",
    "    results.to_csv('results.csv', index=False, mode='a+')  # , header=False\n",
    "\n",
    "\n",
    "atexit.register(save_results)\n",
    "\n",
    "################################################ SET UP ONEDRIVE ACCESS\n",
    "\n",
    "\n",
    "client_secret = 'uzTrTxL5HxBkD=n]PkBg9SQf4N?Lmn5='\n",
    "\n",
    "client_id = '291b2960-9a18-4859-8315-6b099b9ee87a'\n",
    "\n",
    "scopes = ['basic', 'onedrive_all']\n",
    "\n",
    "credentials = (client_id, client_secret)\n",
    "\n",
    "account = Account(credentials)\n",
    "\n",
    "\n",
    "def authenticate():\n",
    "    print('Authenticating...')\n",
    "    if not account.is_authenticated:\n",
    "        account.authenticate(scopes=scopes)\n",
    "    print('Authenticated...')\n",
    "\n",
    "\n",
    "authenticate()\n",
    "\n",
    "storage = account.storage()\n",
    "\n",
    "my_drive = storage.get_default_drive()\n",
    "root_folder = my_drive.get_root_folder()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device: ', device)\n",
    "\n",
    "\n",
    "################################################ IMG SHOW/SAVE\n",
    "\n",
    "\n",
    "def show_im(im):\n",
    "    d = im.shape[-1]\n",
    "    fig, ax = plt.subplots()\n",
    "    im = im.reshape(d, d)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def save_im(im, name='image'):\n",
    "    d = im.shape[-1]\n",
    "    fig, ax = plt.subplots()\n",
    "    im = im.reshape(d, d)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.savefig(name + '.png')\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "################################################ SETTING UP DATASET\n",
    "\n",
    "\n",
    "if not os.path.exists('od_sample_paths.csv'):\n",
    "\n",
    "    data = my_drive.get_item_by_path('/lc_exported_tiles/exported_tiles').get_items()\n",
    "    samples = pd.DataFrame(columns=['ID', 'Img', 'Mask'])\n",
    "    keys = pd.DataFrame(columns=['ID', 'Stroma', 'Immune cells', 'Tumor'])\n",
    "    n = 0\n",
    "    for d in data:\n",
    "        if n % 100 == 0:\n",
    "            print(n, end=' ')\n",
    "        row = {}\n",
    "        k_id = d.name.split('_')[0]\n",
    "        if ').png' in d.name:\n",
    "            row['ID'] = k_id\n",
    "            row['Img'] = d.name\n",
    "            row['Mask'] = os.path.splitext(d.name)[0] + '_mask.png'\n",
    "            samples = samples.append(row, ignore_index=True)\n",
    "        if 'key' in d.name:\n",
    "            print('KF', end=' ')\n",
    "            d.download('', 'key.txt')\n",
    "            with open('key.txt', 'r') as key:\n",
    "                key_row = {}\n",
    "                key_row['ID'] = k_id\n",
    "                key = key.read()\n",
    "                lines = key.split('\\n')\n",
    "                for l in lines:\n",
    "                    sp = l.split('\\t')\n",
    "                    if len(sp) == 2:\n",
    "                        key_row[sp[0]] = int(sp[1])\n",
    "                keys = keys.append(key_row, ignore_index=True)\n",
    "        n += 1\n",
    "\n",
    "    keys.to_csv('sample_keys.csv')\n",
    "    samples = pd.merge(samples, keys, on='ID')\n",
    "    samples.to_csv('od_sample_paths.csv')\n",
    "\n",
    "    print('DONE!')\n",
    "\n",
    "\n",
    "class lc_seg_tiles(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.cell_type = 'Immune cells'\n",
    "        self.samples = pd.read_csv('od_sample_paths.csv')[:3]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s = self.samples.iloc[index]\n",
    "        dim = 256\n",
    "        name = s['Img']\n",
    "\n",
    "        img = my_drive.get_item_by_path('/lc_exported_tiles/exported_tiles/' + s['Img'])\n",
    "        mask = my_drive.get_item_by_path('/lc_exported_tiles/exported_tiles/' + s['Mask'])\n",
    "\n",
    "        img_buffer = io.BytesIO()\n",
    "        mask_buffer = io.BytesIO()\n",
    "\n",
    "        img = img.download(output=img_buffer)\n",
    "        mask = mask.download(output=mask_buffer)\n",
    "\n",
    "        img = np.array(Image.open(img_buffer))[:dim, :dim]\n",
    "\n",
    "        #standardisation\n",
    "        img = (img - np.mean(img)) / np.std(img)\n",
    "        img = np.pad(img, ((0, dim - img.shape[0]), (0, dim - img.shape[1])), 'minimum')\n",
    "\n",
    "        mask = np.array(Image.open(mask_buffer))[:dim, :dim]\n",
    "        mask = np.pad(mask, ((0, dim - mask.shape[0]), (0, dim - mask.shape[1])), 'minimum')\n",
    "\n",
    "        img_buffer.close()\n",
    "        mask_buffer.close()\n",
    "\n",
    "        cell_key = s[self.cell_type]\n",
    "        mask[mask != cell_key] = 0\n",
    "        mask[mask == cell_key] = 1\n",
    "        count = np.sum(mask)\n",
    "\n",
    "        return np.expand_dims(img.astype(np.float32), 0), np.expand_dims(count.astype(np.float32), 0), np.expand_dims(mask.astype(np.float32), 0), name\n",
    "\n",
    "\n",
    "################################################ MODELS\n",
    "\n",
    "\"\"\" Parts of the U-Net model \n",
    "https://github.com/milesial/Pytorch-UNet\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(mid_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "                nn.MaxPool2d(2),\n",
    "                DoubleConv(in_channels, out_channels)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.rn = models.resnet50(num_classes=num_outputs)\n",
    "        self.conv1 = nn.Conv2d(1, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.rn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleFC(nn.Module):\n",
    "\n",
    "    def __init__(self, num_outputs=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 24, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(24, 48, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(48, 96, kernel_size=5, padding=2)\n",
    "        self.conv4 = nn.Conv2d(96, 96, kernel_size=5, padding=2)\n",
    "        self.conv5 = nn.Conv2d(96, 96, kernel_size=5, padding=2)\n",
    "\n",
    "        self.gradients = None\n",
    "\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "\n",
    "        x, _ = x.max(dim=1)\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "        return x\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        return x\n",
    "\n",
    "\n",
    "################################################ TRAINING\n",
    "\n",
    "\n",
    "def run_epochs(net, dataloader, criterion, optimizer, num_epochs, path, save_freq=100, train=True):\n",
    "    global time_taken\n",
    "    global epoch_loss\n",
    "    global epoch\n",
    "    global last_batch_loss\n",
    "    start_time = time.time()\n",
    "    if not train:\n",
    "        net.eval()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i, data in enumerate(dataloader):\n",
    "\n",
    "            inputs, labels, masks, name = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = torch.sigmoid(net(inputs))\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            if True in torch.isnan(loss):\n",
    "                print('NaaaaaaaaaaN!')\n",
    "                return net, epoch_loss\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            time_taken = np.round(time.time() - start_time)\n",
    "            mabs_loss = torch.mean(torch.abs(outputs - masks)).item()\n",
    "            last_batch_loss = (loss / inputs.shape[0]).item()\n",
    "            print('Epoch: {} Batch: {}/{}     Avg Loss: {} MAE: {}'.format(epoch, i + 1, len(dataloader), last_batch_loss, mabs_loss))\n",
    "\n",
    "            epoch_loss += last_batch_loss\n",
    "            if (i + 1) % save_freq == 0:\n",
    "                print('Saving images...')\n",
    "                im = inputs[0].detach().cpu().numpy()\n",
    "                o = np.round(outputs[0].detach().cpu().numpy())\n",
    "                m = masks[0].detach().cpu().numpy()\n",
    "                show_im(im, '{}_input_{}_{}'.format(name, epoch, i))\n",
    "                save_im(o, '{}_output_{}_{}'.format(name, epoch, i))\n",
    "                save_im(m, '{}_mask_{}_{}'.format(name, epoch, i))\n",
    "\n",
    "                if train:\n",
    "                    torch.save(net.state_dict(), path)\n",
    "\n",
    "        print('\\tCompleted epoch {}. Avg epoch loss: {}'.format(epoch, epoch_loss / len(dataloader)))\n",
    "        torch.save(net.state_dict(), args.path)\n",
    "        if epoch_loss < args.conv_thresh:\n",
    "            return net\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net\n",
    "\n",
    "\n",
    "batch_size = args.batch_size\n",
    "dataset = lc_seg_tiles()\n",
    "\n",
    "net = UNet(1, 1)\n",
    "net.to(device)\n",
    "num_epochs = args.epochs\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "\n",
    "train_size = int(0.5 * len(dataset))\n",
    "val_test_size = len(dataset) - train_size\n",
    "val_size = int(0.5 * val_test_size)\n",
    "test_size = val_test_size - val_size\n",
    "\n",
    "train_data, val_test_data = torch.utils.data.random_split(dataset, [train_size, val_test_size])\n",
    "val_data, test_data = torch.utils.data.random_split(val_test_data, [val_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "if args.load:\n",
    "    net.load_state_dict(torch.load(args.path))\n",
    "\n",
    "if args.train:\n",
    "    print('Training network...')\n",
    "    net = run_epochs(net, train_loader, criterion, optimizer, num_epochs, args.path, save_freq=args.save_freq)\n",
    "\n",
    "\n",
    "################################################ VALIDATION\n",
    "\n",
    "else:\n",
    "    print('Evaluating network...')\n",
    "    net.load_state_dict(torch.load(args.path))\n",
    "    _ = run_epochs(net, eval_loader, criterion, None, 1, None, train=False, save_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
